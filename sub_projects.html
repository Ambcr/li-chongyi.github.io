<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Projects</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width">
  </head>
  <body>
    <div class="wrapper">
<header>
<h7>Chongyi Li</h7><br><br>
<div>

<img src="sub_img/IMG_7689.jpg" border="0" width="80%"><br></div><br>
  
<p>
<small>lichongyi25@gmail.com lichongyi@tju.edu.cn</small><br><br>
<a href="https://github.com/Li-Chongyi" target="_blank">[GitHub]</a>  
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Chongyi" target="_blank">[DBLP]</a>  <br>
<a href="https://scholar.google.com/citations?user=1_I0P-AAAAAJ&hl=zh-CN" target="_blank">[Google Scholar]</a> <br>
</p> <br>
<p class="view"><a href="https://li-chongyi.github.io/">Homepage</a></p>
<p class="view"><a href="sub_publication.html">Publications</a></p>
<p class="view"><a href="sub_projects.html">Projects</a></p>
</header>

      <section>

<h2>
<a id="publications-pages" class="anchor" href="#publications-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Projects:</h2>

<hr />

<h3>Emerging from Water: Underwater Image Color Correction Based on Weakly Supervised Color Transfer [<a href="proj_Emerging_water.html"><font color="#0000FF">Details</font></a>]</h3>
<div style="text-align: justify; display: block; margin-right: auto;">
<p>We propose a weakly supervised color transfer method to correct color distortion, which relaxes the need for paired underwater images for training
and allows the underwater images being taken in unknown locations.</p> 
<br>

<hr />
 </div>
<h3>LightenNet: a convolutional neural network for weakly illuminated image enhancement [<a href="proj_lowlight.html"><font color="#0000FF">Details</font></a>]</h3>
<div style="text-align: justify; display: block; margin-right: auto;">
<p>Weak illumination or low light image enhancement as pre-processing is needed in many computer vision asks. Existing methods show limitations when they are used to enhance weakly illuminated images,
especially for the images captured under diverse illumination circumstances. In this letter, we propose  trainable Convolutional Neural Network (CNN) for weakly illuminated image enhancement, namely
LightenNet, which takes a weakly illuminated image as input and outputs its illumination map that is ubsequently used to obtain the enhanced image based on Retinex model. The proposed method produces
visually pleasing results without over or under-enhanced regions. Qualitative and quantitative comparisons are conducted to evaluate the performance of the proposed method. The experimental results
demonstrate that the proposed method achieves superior performance than existing methods. Additionally, we propose a new weakly illuminated image synthesis approach, which can be use as a guide for
weakly illuminated image enhancement networks training and full-reference image quality assessment.</p>   
<br>

<hr />
 
  
<h3>Hierarchical Features Driven Residual Learning for Depth Map Super-Resolution [<a href="proj_SR.html"><font color="#0000FF">Details</font></a>]</h3>
<div style="text-align: justify; display: block; margin-right: auto;">
 <p>Rapid development of affordable and portable consumer depth cameras facilitates the use of depth information in many computer vision tasks such as intelligent vehicles and 3D reconstruction. 
  However, depth map captured by low-cost depth sensors (e.g., Kinect) usually suffers from low spatial resolution, which limits its potential applications. In this paper, we propose a novel 
  deep network for depth map super-resolution (SR), called DepthSR-Net. The proposed DepthSR-Net automatically infers a high resolution (HR) depth map from its low resolution (LR) version by 
  hierarchical features driven residual learning. Specifically, DepthSR-Net is built on a residual U-Net deep network architecture. Given LR depth map, we first obtain the desired HR by bicubic 
  interpolation upsampling, and then construct an input pyramid to achieve multiple level receptive fields. Next, we extract hierarchical features from the input pyramid, intensity image, and 
  encoder-decoder structure of U-Net. Finally, we learn the residual between the interpolated depth map and the corresponding HR one using the rich hierarchical features. The final HR depth map 
  is achieved by adding the learned residual to the interpolated depth map. We conduct an ablation study to demonstrate the effectiveness of each component in the proposed network. Extensive 
  experiments demonstrate that the proposed method outperforms the state-of-the-art methods. Additionally, the potential usage of the proposed network in other low-level vision problems is
  discussed.</p> 
<br>

<hr />
 
  
<h3>An Underwater Image Enhancement Benchmark Dataset and Beyond [<a href="proj_benchmark.html"><font color="#0000FF">Details</font></a>]</h3>
<div style="text-align: justify; display: block; margin-right: auto;">
<p>Underwater image enhancement has been attracting much attention due to its significance in marine engineering and aquatic robot. Numerous underwater image enhancement algorithms have been proposed in the last few years. 
However, these algorithms are mainly evaluated using either synthetic datasets or few selected real-world images. It is thus unclear how these algorithms would perform on images acquired in the wild and how we could gauge 
the progress in the field. To bridge this gap, we present the first comprehensive perceptual study and analysis of underwater image enhancement using large-scale real-world degraded images. In this paper, we construct an 
Underwater Image Enhancement Benchmark Dataset (UIEBD) including 950 real-world underwater images, 890 of which have the corresponding reference images. We treat the rest 60 underwater images which cannot obtain satisfactory 
references as challenging data. Using this dataset, we conduct a comprehensive study of the state-of-the-art underwater image enhancement algorithms qualitatively and quantitatively. In addition, we propose an end-to-end Deep
Underwater Image Enhancement Network (DUIENet) trained on this benchmark as a baseline, which indicates the generalization of the proposed UIEBD for training Convolutional Neural Networks (CNNs). The benchmark evaluations and 
the proposed DUIENet demonstrate the performance and limitations of state-of-the-art algorithms which shed light on the future research in underwater image enhancement.</p>
<br>

<hr />
 


<div class="tg-footer-other-widgets">
   			<div class="tg-second-footer-widget">
   				<aside id="text-4" class="widget widget_text clearfix"><h3 class="widget-title"><span>Who is visiting?</span></h3>			<div class="textwidget"><script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?u=EgJ0&d=jyWeprMnpUZyO_7ty-l_1FnKAKat0AB_nYVXgTXVjSI"></script></div>
      </section>

    </div>
    <script src="../javascripts/scale.fix.js"></script>
  </body>
</html>

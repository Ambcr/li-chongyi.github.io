
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0044)https://nbei.github.io/video-inpainting.html -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="Video-Inpainting.&gt;
&lt;meta name=" keywords"="">

<!-- Fonts and stuff -->
<link href="./dct_iccv2019/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./dct_iccv2019/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./dct_iccv2019/iconize.css">
<script type="text/javascript" async="" src="./dct_iccv2019/ga.js"></script><script type="text/javascript" async="" src="./dct_iccv2019/ga.js.download"></script><script async="" src="./dct_iccv2019/prettify.js.download"></script>

<script type="text/javascript">
            
            var _gaq = _gaq || [];
            _gaq.push(['_setAccount', 'UA-22940424-1']);
            _gaq.push(['_trackPageview']);
            
            (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
            })();
            
</script>

</head>

<body>
  <div id="content">
    <div id="content-inner">
      
      <div class="section head">
	<h2>Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement</h2>

	<div class="authors">
	  <a href="https://penincillin.github.io/">Yu Rong<sup>1</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="https://liuziwei7.github.io/">Ziwei Liu<sup>1</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="https://scholar.google.com/citations?user=F5rVlz0AAAAJ&hl=en">Cheng Li<sup>2</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="https://ai.stanford.edu/~kaidicao/">Kaidi Cao<sup>4</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="http://personal.ie.cuhk.edu.hk/~ccloy/">Chen Change Loy<sup>3</sup></a>
	</div>

	<div class="affiliations">
	  <sup>1</sup> <a href="http://mmlab.ie.cuhk.edu.hk/" target="_blank">CUHK - SenseTime Joint Lab, The Chinese University of Hong Kong</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br>
	  <sup>2</sup> <a href="https://www.sensetime.com/">SenseTime Research</a>&nbsp;&nbsp;
	  <sup>3</sup> <a href="https://www.ntu.edu.sg/Pages/home.aspx">Nanyang Technological University, Singapore</a>&nbsp;&nbsp;
	  <sup>4</sup> <a href="http://svl.stanford.edu/">Stanford University</a>
	</div>

	<div class="venue"></div>
  </div>

    
  <div class="section demo">
		<br> <center>
      <iframe width="840" height="472" src="https://www.youtube.com/embed/8TULGUsb4ZE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> </center> <br>
	</div>

  <div class="section abstract">
	<h2>Abstract</h2>
	<br>
	<p>
		Though much progress has been achieved in single-image 3D human recovery, estimating 3D model for in-the-wild images remains a formidable challenge. The reason lies in the fact that obtaining high-quality 3D annotations for in-the-wild images is an extremely hard task that consumes enormous amount of resources and manpower.  
To tackle this problem, previous methods adopt a hybrid training strategy that exploits multiple heterogeneous types of annotations including 3D and 2D while leaving the efficacy of each annotation not thoroughly investigated.
In this work, we aim to perform a comprehensive study on cost and effectiveness trade-off between different annotations.
Specifically, we focus on the challenging task of in-the-wild 3D human recovery from single images when paired 3D annotations are not fully available.
Through extensive experiments, we obtain several observations: 1) 3D annotations are efficient, whereas traditional 2D annotations such as 2D keypoints and body part segmentation are less competent in guiding 3D human recovery. 2) Dense Correspondence such as DensePose is effective. When there are no paired in-the-wild 3D annotations available, models exploiting dense correspondence can achieve 92% of the performance compared to a model trained with paired 3D data.
We show that incorporating dense correspondence into in-the-wild 3D human recovery is promising and competitive due to its high efficiency and relatively low annotating cost. Our model trained with dense correspondence can serve as a strong reference for future research.

    <br><br>
	</p>
      </div>
	<div class="section framework">
	<h2>Framework</h2>
    <br>
	<center><img src="./dct_iccv2019/framework.png" border="0" width="95%"></center>
	<p>
     Our framework is composed of three components. 1) Input encoding part takes inputs and outputs encoded features. 2) Parameter estimator estimates the pose and shape parameters of the SMPL model given the outputs of the encoder. 3) Given estimated parameters, SMPL model generates predicted 3D joints, 2D keypoints and dense keypoints to calculate loss. Figure (b) shows two possible architectures of the input encoder. Input encoder could either be composed of a single branch that only takes one kind of inputs or two branches that takes original images and the other auxiliary inputs.
	</p>
    </div>

    <br>
    <div class="section experiment">
        <h2>Experimental Results</h2>
        <br>
            <center><img src="./dct_iccv2019/table.png" border="0" width="100%"></center>
        <p> The evaluation metrics are PVE, MPJPE and PVE-T, separately. For all metrics, lower is better. "3D" refers to paired in-the-wild 3D annotations. "20% 3D" refers to 20% randomly selected 3D annotations. "Sparse 2D" refers to sparse 2D keypoints. "Dense" refers to dense correspondence, namely, IUV maps generated by DensePose.  </p>
        <center><img src="./dct_iccv2019/efficiency_3d.png" border="0" width="100%"></center>
    </div>
        

    <br><br>
    <div class="section visualization">
        <h2>Visualization</h2>
        <br>
            <center><img src="./dct_iccv2019/visualization.png" border="0" width="100%"></center>
        <p>
        Results of our flow-guided video inpainting approach. For each input sequence (odd row), 
        we show representative frames with mask of missing region overlay. We show the inpainting 
        results in even rows.
        </p>
    </div>
	
    <br><br>
<div class="section materials">
	<h2>Materials</h2>
	<center>
	  <ul>

          <li class="grid">
	      <div class="griditem">
		<a href="https://arxiv.org/abs/1908.06442" target="_blank" class="imageLink"><img src="./dct_iccv2019/paper.png" border="0" width="40%"></a><br>
		  <a href="https://arxiv.org/abs/1908.06442" target="_blank">Paper</a>
		</div>
	      </li>
		  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

        <li class="grid">
	      <div class="griditem">
		<a href="dct_iccv2019/DCT_supp.pdf" target="_blank" class="imageLink"><img src="./dct_iccv2019/supp.png" border="0" width="40%"></a><br>
		  <a href="dct_iccv2019/DCT_supp.pdf" target="_blank">Supplemental Material</a>
		</div>
	      </li>
		  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	
		  
		  <li class="grid">
	      <div class="griditem">
		<a href="https://github.com/penincillin/DCT_ICCV-2019"><img src="./dct_iccv2019/code.png"></a><br>
		  <a href="https://github.com/penincillin/DCT_ICCV-2019">Code and Models</a>
		</div>
	      </li>
	  
	    </ul>
	    </center>
	    </div>
	    
        <br><br>
<div class="section citation">
	<h2>Citation</h2>
	<div class="section bibtex">
	  <pre>@InProceedings{Rong_2019_ICCV,
author = {Rong, Yu and Liu, Ziwei and Li, Cheng and Cao, Kaidi and Change Loy, Chen},
title = {Delving Deep into Hybrid Annotations for 3D Human Recovery in the Wild},
booktitle = {Proceedings of the IEEE international conference on computer vision},
year = {2019}
}</pre>
	  </div>
      </div>

</div></div>
</body></html>
